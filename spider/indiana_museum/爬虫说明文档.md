# 爬虫说明文档
作者：程志凯，张芊铌
## 一、需求分析
* 待爬取网站：Sidney and Lois Eskenazi Museum of Art：https://artmuseum.indiana.edu/index.html

* 数据格式要求
所有爬取的数据以utf-8编码保存为csv格式。建议使用csv，pandas等包进行读写，防止出现格式错误等问题。各组应自行检验导出csv文件可正常使用。

* 数据内容要求
应至少包含以下数据，文物基础数据（网站提供），如标题、时代、介绍等；文物对应的详情页面的URL；文物图片（原图，爬取请注意检查，部分网站可能需要手动解析原图地址）；原图下载链接。
需要提交一个说明文档，包括英文字段对应的中文说明，如time：文物时代。

## 二、csv文档格式说明
csv文档共包含7列。
第一列为Id，代表文物序号。
第二列为name，代表文物的标题。
第三列为author，代表文物的作者。
第四列为relicTime，代表文物所处的朝代。
第五列为imageUrl，代表图片下载地址。
第六列为description，代表文物的介绍。
第七列为realUrl，代表该文物的网址。
其中，第一、二、四、六、七列数据必定存在。
第三列数据可能为空（如果无作者，在csv中填入的为空）。第五列数据可能为空（如果无图片，在csv中填入的为空）。

## 三、代码技术
request 请求html，再使用beautifulsuop和正则解析

## 四、代码注解
### 1.网页结构分析
#### 1.1 总览网页
通过手动点击导航栏的Collection online
点击进去便可以进入藏品页面。
#### 1.2 搜索藏品网页
网站并没有直接列出来藏品，需要搜索得到，不过输入栏为空时可以搜索到全部藏品
#### 1.3 文物介绍页面
在搜索出来的结果中，点进去便是文物详情页面
网站由法语编写，最上面最粗体的字是文物的标题。然后下面一行是文物的作者。下面几段文字是文物的介绍。页面的左侧是文物的图片，大部分文物没有图片。

### 2.主函数讲解
```
#多线程
with ThreadPoolExecutor(max_workers=10) as executor:
    executor.map(process_page, range(36,1160))
```
```
#请求id与解析
def process_page(i):
    url = "https://artmuseum.indiana.edu/collections-online/browse/results.php?keyword=&page=" + str(i)
    response = requests.get(url,headers=head)
    data = response.text
    matchs = []
    soup = BeautifulSoup(data, 'html.parser')
    ass = soup.find_all('div', class_='search-result')
    pattern = re.compile(r'flex" href="object.php\?number=([^"]*)"') # 匹配所有的小数
    for a in ass: 
        match = pattern.findall(str(a))
        matchs.append(match[0])
        
    print(i)
    #保存matches
    with open("matchs.txt", "a", encoding="utf-8") as fp:
        for i in matchs:
            json.dump(i, fp, ensure_ascii=False, indent=4)
            fp.write('\n')
```
通过id来获取文物详情页面
```
#请求item与解析
def process_item(i):
    print(i)
    info = {}
    url = "https://artmuseum.indiana.edu/collections-online/browse/object.php?number=" + matchs[i][1:-1]
    info['realUrl'] = url
    response = requests.get(url,headers=head)
    data = response.text
    soup = BeautifulSoup(data, 'html.parser')
    # get info
    tombstone_lines = soup.find_all('div', class_='tombstone-line')
    pattern_key = re.compile(r'<strong>(.*?)</strong>') 
    pattern_value = re.compile(r'</span>\s*(.*?)\s*</div>', re.DOTALL)
    for j in tombstone_lines:
        key = pattern_key.findall(str(j))
        value = pattern_value.findall(str(j))
        # print(key[0],value)   
        if(key[0] == 'Title'): 
            value[0] = value[0].replace('<i>','')
            value[0] = value[0].replace('</i>','')
            key[0] = 'name'
        elif(key[0] == 'Accession Number'):
            key[0] = 'Id'
        elif(key[0] == 'Artist'):
            key[0] = 'author'
        elif(key[0] == 'Date'):
            key[0] = 'relicTime'
        elif(key[0] == 'Credit Line'):
            key[0] = 'description'
        else:
            key[0] = 'abort'
        if(key[0] != 'abort'):
            value[0] = value[0].replace('\n', '')
            value[0] = value[0].replace('\r', '')
            value[0] = value[0].strip()
            info[key[0]] = value[0]
    # 写入CSV文件
    with open('output.csv', 'a', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writerow(info)
```